{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic imports for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 21:37:10.180921: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports that will emulate the game and an action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import retro"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports for our PPO algorithm (Proximal Policy Optimization) and for pre-processing (grayscale and frame stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GPU for training (with AMD's ROCm), checking if we have access to Cuda (at least 4 times faster training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hijacking the default action space with a wrapper that will allow us to\n",
    "### create our own combos of buttons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discretizer(gym.ActionWrapper):\n",
    "\n",
    "    def __init__(self, env, combos):\n",
    "        super().__init__(env)\n",
    "        assert isinstance(env.action_space, gym.spaces.MultiBinary)\n",
    "        buttons = env.unwrapped.buttons\n",
    "        self._decode_discrete_action = []\n",
    "        for combo in combos:\n",
    "            arr = np.array([False]*env.action_space.n)\n",
    "            for button in combo:\n",
    "                arr[buttons.index(button)] = True\n",
    "            self._decode_discrete_action.append(arr)\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(len(self._decode_discrete_action))\n",
    "\n",
    "    def action(self, act):\n",
    "        return self._decode_discrete_action[act].copy()\n",
    "\n",
    "\n",
    "class MaRLioDiscretizer(Discretizer):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env=env, \n",
    "                         combos=[\n",
    "            [],\n",
    "            [\"RIGHT\"],\n",
    "            [\"RIGHT\",\"Y\"],\n",
    "            [\"RIGHT\",\"A\"],\n",
    "            [\"RIGHT\",\"Y\",\"A\"],\n",
    "            [\"RIGHT\",\"Y\",\"B\"],\n",
    "            [\"LEFT\",\"Y\"],\n",
    "            [\"LEFT\",\"Y\",\"A\"],\n",
    "            [\"LEFT\",\"Y\",\"B\"],\n",
    "            [\"A\"],\n",
    "            [\"B\"]\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping our combos in a list for an easy visualization using indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "combos = [\n",
    "    [],\n",
    "    [\"RIGHT\"],\n",
    "    [\"RIGHT\",\"Y\"],\n",
    "    [\"RIGHT\",\"A\"],\n",
    "    [\"RIGHT\",\"Y\",\"A\"],\n",
    "    [\"RIGHT\",\"Y\",\"B\"],\n",
    "    [\"LEFT\",\"Y\"],\n",
    "    [\"LEFT\",\"Y\",\"A\"],\n",
    "    [\"LEFT\",\"Y\",\"B\"],\n",
    "    [\"A\"],\n",
    "    [\"B\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the path of our custom data.json and scenario.json files into variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data.json\"\n",
    "scenario_path = \"./scenario.json\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data.json will have memory addresses for things that we want to keep track of\n",
    "### and will be accessed from the info variable once we run the environment\n",
    "### scenario.json will be our way of calculating MaRLio's reward\n",
    "### Let's see how they look like. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data.json\n",
    "\n",
    "````\n",
    "{\n",
    "  \"info\": {\n",
    "    \"checkpoint\": {\n",
    "      \"address\": 5070,\n",
    "      \"type\": \"|i1\"\n",
    "    },\n",
    "    \"coins\": {\n",
    "      \"address\": 8261055,\n",
    "      \"type\": \"|u1\"\n",
    "    },\n",
    "    \"end_of_level\": {\n",
    "      \"address\": 8259846,\n",
    "      \"type\": \"|i1\"\n",
    "    },\n",
    "    \"lives\": {\n",
    "      \"address\": 8261054,\n",
    "      \"type\": \"|i1\"\n",
    "    },\n",
    "    \"powerups\": {\n",
    "      \"address\": 25,\n",
    "      \"type\": \"|i1\"\n",
    "    },\n",
    "    \"score\": {\n",
    "      \"address\": 8261428,\n",
    "      \"type\": \"<u4\"\n",
    "    },\n",
    "    \"xpos\": {\n",
    "      \"address\": 148,\n",
    "      \"type\": \"<u2\"\n",
    "    },\n",
    "    \"ypos\": {\n",
    "      \"address\": 114,\n",
    "      \"type\": \"<u4\"\n",
    "    },\n",
    "    \"yoshi_coins\": {\n",
    "      \"address\": 8262690,\n",
    "      \"type\": \"<u4\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "````"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scenario.json\n",
    "\n",
    "````\n",
    "{\n",
    "  \"crop\": [\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0\n",
    "  ],\n",
    "  \"done\": {\n",
    "    \"variables\": {\n",
    "      \"lives\": {\n",
    "        \"op\": \"equal\",\n",
    "        \"reference\": -1\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"reward\": {\n",
    "    \"variables\": {\n",
    "      \"score\": {\n",
    "        \"reward\": 1.0\n",
    "      },\n",
    "      \"coins\": {\n",
    "        \"reward\": 1.0\n",
    "      },\n",
    "      \"yoshi_coins\": {\n",
    "        \"reward\": 1.0\n",
    "      },\n",
    "      \"xpos\": {\n",
    "        \"op\": \"greater\",\n",
    "        \"reference\": \"last\",\n",
    "        \"reward\": 10\n",
    "      },\n",
    "      \"ypos\": {\n",
    "        \"op\": \"less\",\n",
    "        \"reference\": \"last\",\n",
    "        \"reward\": 10\n",
    "      },\n",
    "      \"xpos\": {\n",
    "        \"op\": \"less\",\n",
    "        \"reference\": \"last\",\n",
    "        \"penalty\": 0.1\n",
    "      },\n",
    "      \"lives\": {\n",
    "        \"op\": \"less\",\n",
    "        \"reference\": \"last\",\n",
    "        \"penalty\": 1000.0\n",
    "      },\n",
    "      \"end_of_level\": {\n",
    "        \"op\": \"equal\",\n",
    "        \"reference\": 1,\n",
    "        \"reward\": 1000000.0\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "````"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's load our game, Super Mario World (SMW) from the Super Nintendo Entertainment System (SNES)\n",
    "### using scenario and info kwargs, we can load our custom data.json and scenario.json\n",
    "### using record kwarg, we can save the training sessions for future viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Failed to load info (./data.json) or scenario (./scenario.json)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# for train\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m env \u001b[39m=\u001b[39m retro\u001b[39m.\u001b[39;49mmake(game\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mSuperMarioWorld-Snes\u001b[39;49m\u001b[39m\"\u001b[39;49m, state\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mYoshiIsland3\u001b[39;49m\u001b[39m\"\u001b[39;49m, record\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./recordings/train/\u001b[39;49m\u001b[39m\"\u001b[39;49m, scenario\u001b[39m=\u001b[39;49mscenario_path, info\u001b[39m=\u001b[39;49mdata_path)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/retro/__init__.py:55\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(game, state, inttype, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mGame not found: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m. Did you make sure to import the ROM?\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m game)\n\u001b[0;32m---> 55\u001b[0m \u001b[39mreturn\u001b[39;00m RetroEnv(game, state, inttype\u001b[39m=\u001b[39;49minttype, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/retro/retro_env.py:96\u001b[0m, in \u001b[0;36mRetroEnv.__init__\u001b[0;34m(self, game, state, scenario, info, use_restricted_actions, record, players, inttype, obs_type)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_buttons \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuttons)\n\u001b[1;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mload(info_path, scenario_path), \u001b[39m'\u001b[39m\u001b[39mFailed to load info (\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) or scenario (\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (info_path, scenario_path)\n\u001b[1;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mem\n",
      "\u001b[0;31mAssertionError\u001b[0m: Failed to load info (./data.json) or scenario (./scenario.json)"
     ]
    }
   ],
   "source": [
    "# for train\n",
    "\n",
    "env = retro.make(game=\"SuperMarioWorld-Snes\", state=\"YoshiIsland3\", record=\"./recordings/train/\", scenario=scenario_path, info=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pred\n",
    "\n",
    "# env = retro.make(game=\"SuperMarioWorld-Snes\", state=\"YoshiIsland3\", record=\"./recordings/pred/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test\n",
    "\n",
    "# env = retro.make(game=\"SuperMarioWorld-Snes\", state=\"YoshiIsland3\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is where we wrap the environment in our own action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_env = MaRLioDiscretizer(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = disc_env.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the index it returns and what combo we get from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = disc_env.action_space.sample()\n",
    "print(action)\n",
    "combos[action]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can use that action inside our environment, one frame at a time\n",
    "### and we can use tuple unpacking, to get access to some important information about our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, reward, terminated, info = disc_env.step(action)\n",
    "print(f\"score: {reward}\\nterminated: {terminated}\\ninfo: {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The image we get is a 3-dimensional tensor with a height of 224 pixels, a width of 256 pixels, and 3 color channels (RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize that frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# game loop with random actions\n",
    "# state = disc_env.reset()\n",
    "\n",
    "# done = False\n",
    "# while not done:\n",
    "#     action = disc_env.action_space.sample()\n",
    "#     state, reward, done, info = disc_env.step(action)\n",
    "#     disc_env.render()\n",
    "#     time.sleep(0.005)\n",
    "# disc_env.render(close=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's pre-process our images\n",
    "### First, grayscale. that'll reduce the size of our environment by 66%\n",
    "### Second, let's vectorize everything\n",
    "### Third, let's stack 4 frames, our model will get a sense of momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_env = GrayScaleObservation(disc_env, keep_dim=True)\n",
    "disc_env = DummyVecEnv([lambda:disc_env])\n",
    "disc_env = VecFrameStack(disc_env, 4, channels_order=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = disc_env.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at a frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(state[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This time, the (1, 224, 256, 4) shape indicates that the observation is a stack of 4 frames, \n",
    "### where each frame is a \"color\" (grayscale, 1 RGB channel) image with a resolution of 224x256 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking 4 steps, so we don't have blank frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(3):\n",
    "    state, reward, done, info = disc_env.step([disc_env.action_space.sample()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize our stack of frames, with random actions from our list of combos\n",
    "### We should see some movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,16))\n",
    "for idx in range(state.shape[3]):\n",
    "    plt.subplot(1,4,idx+1)\n",
    "    plt.imshow(state[0][:,:,idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = disc_env.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a class that will create our logs and save our model at every check_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback helper function\n",
    "\n",
    "# logs and model saving\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, \"best_model_{}\".format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating variables that points to our paths for saving the model and the logs (tensorboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = \"./train/\"\n",
    "LOG_DIR = \"./logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=10_000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping track of some hyperparameters for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda params\n",
    "\n",
    "# learning_rate = lambda f:f*0.000001\n",
    "learning_rate = 0.000001\n",
    "n_steps = 128\n",
    "clip_range = 0.2\n",
    "ent_coef = 0.001"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance of our PPO algorithm, with the rest of the hyperparameters set to default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    \"CnnPolicy\", \n",
    "    disc_env, \n",
    "    learning_rate=learning_rate, \n",
    "    n_steps=n_steps, \n",
    "    batch_size=64, \n",
    "    n_epochs=10, \n",
    "    gamma=0.99, \n",
    "    gae_lambda=0.95, \n",
    "    clip_range=clip_range, \n",
    "    clip_range_vf=None, \n",
    "    normalize_advantage=True, \n",
    "    ent_coef=ent_coef, \n",
    "    vf_coef=0.5, \n",
    "    max_grad_norm=0.5, \n",
    "    use_sde=False, \n",
    "    sde_sample_freq=-1, \n",
    "    target_kl=None,  \n",
    "    tensorboard_log=LOG_DIR, \n",
    "    policy_kwargs=None, \n",
    "    verbose=0, \n",
    "    seed=42, \n",
    "    device='auto', \n",
    "    _init_setup_model=True\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is where MaRLio starts his adventure (training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(\n",
    "    total_timesteps=100_000, \n",
    "    progress_bar=True, \n",
    "    tb_log_name=\"ppo\",\n",
    "    callback=[\n",
    "    callback\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's load the models and predict the environment with the models' knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marlio = \"best_model_100000.zip\"\n",
    "model = PPO.load(f\"./train/{marlio}\")\n",
    "print(f\"using {marlio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = disc_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# game loop for predict\n",
    "\n",
    "total_reward = 0\n",
    "action_list = [0]\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    action, _ = model.predict(state)\n",
    "    if action[0] not in action_list:\n",
    "        action_list.pop()\n",
    "        action_list.append(action[0])\n",
    "        print(combos[action_list[0]])\n",
    "    state, reward, done, info = disc_env.step(action)\n",
    "    print(info[0][\"ypos\"])\n",
    "    print(info[0][\"xpos\"])\n",
    "    total_reward += reward\n",
    "    disc_env.render()\n",
    "    time.sleep(0.004)\n",
    "\n",
    "print(\"Total reward:\", total_reward)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario.json -> https://retro.readthedocs.io/en/latest/integration.html?highlight=scenario#scenario-scenario-json\n",
    "# metadata.json -> https://retro.readthedocs.io/en/latest/integration.html?highlight=scenario#metadata-json\n",
    "# data.json -> https://retro.readthedocs.io/en/latest/integration.html?highlight=scenario#data-json\n",
    "\n",
    "# appendix for operations -> https://retro.readthedocs.io/en/latest/integration.html?highlight=scenario#appendix-operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BINGO\n",
    "\n",
    "# https://retro.readthedocs.io/en/latest/python.html#render-to-video\n",
    "# python3.8 -m retro.scripts.playback_movie SuperMarioWorld-Snes-YoshiIsland3-001490.bk2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below code not used for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible playback code below\n",
    "\n",
    "movie = retro.Movie(\"./recordings/train/SuperMarioWorld-Snes-YoshiIsland3-000008.bk2\")\n",
    "movie.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = retro.make(\n",
    "    game=movie.get_game(),\n",
    "    state=None,\n",
    "    # bk2s can contain any button presses, so allow everything\n",
    "    use_restricted_actions=retro.Actions.ALL,\n",
    "    players=movie.players,\n",
    ")\n",
    "env.initial_state = movie.get_state()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while movie.step():\n",
    "    keys = []\n",
    "    for p in range(movie.players):\n",
    "        for i in range(env.num_buttons):\n",
    "            keys.append(movie.get_key(i, p))\n",
    "    env.step(keys)\n",
    "    env.render()\n",
    "    # time.sleep(0.004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_video(env, policy, out_directory, fps=30):\n",
    "    images = []\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "    images.append(img)\n",
    "    while not done:\n",
    "        state = torch.Tensor(state).to(device)\n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action, _, _, _ = policy.get_action_and_value(state)\n",
    "        state, reward, done, info = env.step(\n",
    "            action.cpu().numpy()\n",
    "        )  # We directly put next_state = state for recording logic\n",
    "        img = env.render(mode=\"rgb_array\")\n",
    "        images.append(img)\n",
    "    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b3ded1ccb95c1d9bd405e7b823d9e85424cde40fbb5985eb47e999ef50e15b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
