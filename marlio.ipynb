{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic imports for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports that will emulate the game and an action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import retro"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports for our PPO algorithm (Proximal Policy Optimization) and for pre-processing (grayscale and frame stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GPU for training (with AMD's ROCm), checking if we have access to Cuda (at least 4 times faster training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hijacking the default action space with a wrapper that will allow us to\n",
    "### create our own combos of buttons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combos = [\n",
    "    [],\n",
    "    [\"RIGHT\"],\n",
    "    [\"RIGHT\",\"Y\"],\n",
    "    [\"RIGHT\",\"A\"],\n",
    "    [\"RIGHT\",\"Y\",\"A\"],\n",
    "    [\"RIGHT\",\"Y\",\"B\"],\n",
    "    [\"LEFT\",\"Y\"],\n",
    "    [\"LEFT\",\"Y\",\"A\"],\n",
    "    [\"LEFT\",\"Y\",\"B\"],\n",
    "    [\"A\"],\n",
    "    [\"B\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discretizer(gym.ActionWrapper):\n",
    "\n",
    "    def __init__(self, env, combos):\n",
    "        super().__init__(env)\n",
    "        assert isinstance(env.action_space, gym.spaces.MultiBinary)\n",
    "        buttons = env.unwrapped.buttons\n",
    "        self._decode_discrete_action = []\n",
    "        for combo in combos:\n",
    "            arr = np.array([False]*env.action_space.n)\n",
    "            for button in combo:\n",
    "                arr[buttons.index(button)] = True\n",
    "            self._decode_discrete_action.append(arr)\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(len(self._decode_discrete_action))\n",
    "\n",
    "    def action(self, act):\n",
    "        return self._decode_discrete_action[act].copy()\n",
    "\n",
    "\n",
    "class MaRLioDiscretizer(Discretizer):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env=env, combos=combos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the path of our custom data.json and scenario.json files into variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data.json\"\n",
    "scenario_path = \"./scenario.json\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data.json will have memory addresses for things that we want to keep track of\n",
    "### and will be accessed from the info variable once we run the environment\n",
    "### scenario.json will be our way of calculating MaRLio's reward\n",
    "### Let's see how they look like. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data.json\n",
    "\n",
    "````\n",
    "{\n",
    "  \"info\": {\n",
    "    \"checkpoint\": {\n",
    "      \"address\": 5070,\n",
    "      \"type\": \"|i1\"\n",
    "    },\n",
    "    \"coins\": {\n",
    "      \"address\": 8261055,\n",
    "      \"type\": \"|u1\"\n",
    "    },\n",
    "    \"end_of_level\": {\n",
    "      \"address\": 8259846,\n",
    "      \"type\": \"|i1\"\n",
    "    },\n",
    "    \"lives\": {\n",
    "      \"address\": 8261054,\n",
    "      \"type\": \"|i1\"\n",
    "    },\n",
    "    \"powerups\": {\n",
    "      \"address\": 25,\n",
    "      \"type\": \"|i1\"\n",
    "    },\n",
    "    \"score\": {\n",
    "      \"address\": 8261428,\n",
    "      \"type\": \"<u4\"\n",
    "    },\n",
    "    \"xpos\": {\n",
    "      \"address\": 148,\n",
    "      \"type\": \"<u2\"\n",
    "    },\n",
    "    \"ypos\": {\n",
    "      \"address\": 114,\n",
    "      \"type\": \"<u4\"\n",
    "    },\n",
    "    \"yoshi_coins\": {\n",
    "      \"address\": 8262690,\n",
    "      \"type\": \"<u4\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "````"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scenario.json\n",
    "\n",
    "````\n",
    "{\n",
    "  \"crop\": [\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0\n",
    "  ],\n",
    "  \"done\": {\n",
    "    \"variables\": {\n",
    "      \"lives\": {\n",
    "        \"op\": \"equal\",\n",
    "        \"reference\": -1\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"reward\": {\n",
    "    \"variables\": {\n",
    "      \"score\": {\n",
    "        \"reward\": 1.0\n",
    "      },\n",
    "      \"coins\": {\n",
    "        \"reward\": 1.0\n",
    "      },\n",
    "      \"yoshi_coins\": {\n",
    "        \"reward\": 1.0\n",
    "      },\n",
    "      \"xpos\": {\n",
    "        \"op\": \"greater\",\n",
    "        \"reference\": \"last\",\n",
    "        \"reward\": 10\n",
    "      },\n",
    "      \"ypos\": {\n",
    "        \"op\": \"less\",\n",
    "        \"reference\": \"last\",\n",
    "        \"reward\": 10\n",
    "      },\n",
    "      \"xpos\": {\n",
    "        \"op\": \"less\",\n",
    "        \"reference\": \"last\",\n",
    "        \"penalty\": 0.1\n",
    "      },\n",
    "      \"lives\": {\n",
    "        \"op\": \"less\",\n",
    "        \"reference\": \"last\",\n",
    "        \"penalty\": 1000.0\n",
    "      },\n",
    "      \"end_of_level\": {\n",
    "        \"op\": \"equal\",\n",
    "        \"reference\": 1,\n",
    "        \"reward\": 1000000.0\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "````"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's load our game, Super Mario World (SMW) from the Super Nintendo Entertainment System (SNES)\n",
    "### using scenario and info kwargs, we can load our custom data.json and scenario.json\n",
    "### using record kwarg, we can save the training sessions for future viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves recording to ./recordings/train/\n",
    "\n",
    "env = retro.make(game=\"SuperMarioWorld-Snes\", state=\"YoshiIsland1\", record=\"./recordings/train/\", scenario=scenario_path, info=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves recording to ./recordings/pred/\n",
    "\n",
    "# env = retro.make(game=\"SuperMarioWorld-Snes\", state=\"YoshiIsland1\", record=\"./recordings/pred/\", scenario=scenario_path, info=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for video only, no recording\n",
    "\n",
    "# env = retro.make(game=\"SuperMarioWorld-Snes\", state=\"YoshiIsland1\", scenario=scenario_path, info=data_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is where we wrap the environment in our own action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_env = MaRLioDiscretizer(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = disc_env.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the index it returns and what combo we get from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = disc_env.action_space.sample()\n",
    "print(action)\n",
    "combos[action]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can use that action inside our environment, one frame at a time\n",
    "### and we can use tuple unpacking, to get access to some important information about our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, reward, terminated, info = disc_env.step(action)\n",
    "print(f\"score: {reward}\\nterminated: {terminated}\\ninfo: {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The image we get is a 3-dimensional tensor with a height of 224 pixels, a width of 256 pixels, and 3 color channels (RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize that frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(obs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at Mario doing random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = disc_env.reset()\n",
    "\n",
    "# done = False\n",
    "# for _ in range(1000):\n",
    "#     action = disc_env.action_space.sample()\n",
    "#     state, reward, done, info = disc_env.step(action)\n",
    "#     disc_env.render()\n",
    "#     time.sleep(0.005)\n",
    "# disc_env.render(close=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's pre-process our images\n",
    "### First, grayscale. that'll reduce the size of our environment by 66%\n",
    "### Second, let's vectorize everything\n",
    "### Third, let's stack 4 frames, our model will get a sense of momentum\n",
    "### Normalize the pixels, between 0-1 instead of 0-255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(NormalizeObservation, self).__init__(env)\n",
    "    \n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_env = GrayScaleObservation(disc_env, keep_dim=True)\n",
    "disc_env = DummyVecEnv([lambda:disc_env])\n",
    "disc_env = VecFrameStack(disc_env, 4, channels_order=\"last\")\n",
    "disc_env = NormalizeObservation(disc_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = disc_env.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at a frame (grayscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(state[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This time, the (1, 224, 256, 4) shape indicates that the observation is a stack of 4 frames, \n",
    "### where each frame is a \"color\" (grayscale, 1 RGB channel) image with a resolution of 224x256 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking 4 steps, so we don't have blank frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(3):\n",
    "    state, reward, done, info = disc_env.step([disc_env.action_space.sample()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize our stack of frames, with random actions from our list of combos\n",
    "### We should see some movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,16))\n",
    "for idx in range(state.shape[3]):\n",
    "    plt.subplot(1,4,idx+1)\n",
    "    plt.imshow(state[0][:,:,idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = disc_env.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a class that will create our logs and save our model at every check_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback helper function\n",
    "\n",
    "# logs and model saving\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, \"best_model_{}\".format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating variables that points to our paths for saving the model and the logs (tensorboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = \"./train/\"\n",
    "LOG_DIR = \"./logs/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proximal Policy Optimization (PPO) is a reinforcement learning algorithm that is used to optimize policies for continuous control problems.  \n",
    "#### The PPO algorithm uses a surrogate objective function to update the policy parameters in an iterative manner.  \n",
    "\n",
    "#### The formula for the PPO objective function is as follows:\n",
    "\n",
    "    LCLIP(θ) = E[min(r(θ)A, clip(r(θ), 1 - ε, 1 + ε)A)]\n",
    "\n",
    "#### Where:\n",
    "\n",
    "    θ represents the policy parameters\n",
    "    r(θ) represents the ratio of the probabilities of taking the action under the new policy and the old policy\n",
    "    A represents the advantage function, which estimates how much better a particular action is compared to the average action\n",
    "    ε is a hyperparameter that determines the degree of clipping that is applied to the ratio r(θ)\n",
    "\n",
    "#### The PPO algorithm maximizes this objective function by updating the policy parameters using stochastic gradient ascent.  \n",
    "#### In addition, the algorithm includes an entropy bonus term to encourage exploration and prevent the policy from becoming too deterministic."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](ppo.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping track of some hyperparameters for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_freq = 100_000\n",
    "total_timesteps = 1_000_000\n",
    "\n",
    "learning_rate = lambda f:f*0.000001\n",
    "# learning_rate = 0.000001\n",
    "n_steps = 256\n",
    "clip_range = 0.2\n",
    "ent_coef = 0.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance of our PPO algorithm, with the rest of the hyperparameters set to default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=check_freq, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    \"CnnPolicy\", \n",
    "    disc_env, \n",
    "    learning_rate=learning_rate, \n",
    "    n_steps=n_steps, \n",
    "    batch_size=64, \n",
    "    n_epochs=10, \n",
    "    gamma=0.99, \n",
    "    gae_lambda=0.95, \n",
    "    clip_range=clip_range, \n",
    "    clip_range_vf=None, \n",
    "    normalize_advantage=True, \n",
    "    ent_coef=ent_coef, \n",
    "    vf_coef=0.5, \n",
    "    max_grad_norm=0.5, \n",
    "    use_sde=False, \n",
    "    sde_sample_freq=-1, \n",
    "    target_kl=None,  \n",
    "    tensorboard_log=LOG_DIR, \n",
    "    policy_kwargs=None, \n",
    "    verbose=0, \n",
    "    seed=42, \n",
    "    device='auto', \n",
    "    _init_setup_model=True\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is where MaRLio starts his adventure (training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(\n",
    "    total_timesteps=total_timesteps, \n",
    "    progress_bar=True, \n",
    "    tb_log_name=\"ppo\",\n",
    "    callback=[\n",
    "    callback\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's add a powerup dictionnary, we will see what powerups MaRLio has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "powerup_dict = {\n",
    "    0: \"small\",\n",
    "    1: \"big\",\n",
    "    2: \"cape\",\n",
    "    3: \"fire\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's add a dictionnary of our overlays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_images = {\n",
    "    0: \"./controller/blank.png\",\n",
    "    1: \"./controller/right.png\",\n",
    "    2: \"./controller/righty.png\",\n",
    "    3: \"./controller/righta.png\",\n",
    "    4: \"./controller/rightya.png\",\n",
    "    5: \"./controller/rightyb.png\",\n",
    "    6: \"./controller/lefty.png\",\n",
    "    7: \"./controller/leftya.png\",\n",
    "    8: \"./controller/leftyb.png\",\n",
    "    9: \"./controller/a.png\",\n",
    "    10: \"./controller/b.png\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's load the models and predict the environment with the models' knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# game loop for predict\n",
    "\n",
    "def pred(model):\n",
    "    total_reward = 0\n",
    "    action_dict = {}\n",
    "    powerup = \"\"\n",
    "    check = False\n",
    "    frame = 1\n",
    "\n",
    "    state = disc_env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = model.predict(state)\n",
    "\n",
    "        action_dict[frame] = action[0]\n",
    "        state, reward, done, info = disc_env.step(action)\n",
    "\n",
    "        if info[0][\"powerups\"] != powerup:\n",
    "            powerup = info[0][\"powerups\"]\n",
    "            print(powerup_dict.get(powerup, \"Invalid integer value\"))\n",
    "\n",
    "        if info[0][\"checkpoint\"] == 1 and not check:\n",
    "            check = True\n",
    "            print(\"Midlevel Checkpoint Reached: +500k reward\")\n",
    "\n",
    "        if info[0][\"end_of_level\"] == 1:\n",
    "            print(\"End of level Reached: +1m reward\")\n",
    "\n",
    "        frame += 1\n",
    "\n",
    "        total_reward += reward\n",
    "        disc_env.render()\n",
    "        time.sleep(0.005)\n",
    "\n",
    "    print(\"Total reward:\", total_reward[0])\n",
    "    print()\n",
    "    return action_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function that will convert our .bk2 to .mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bk2_to_mp4():\n",
    "    bk2_files = [f for f in os.listdir('./recordings/pred/') if f.endswith('.bk2')]\n",
    "    bk2_files.sort()\n",
    "    for bk2_file in bk2_files:\n",
    "        print(f\"converting {bk2_file} into .mp4\")\n",
    "        command = f\"python3.8 -m retro.scripts.playback_movie ./recordings/pred/videos/{bk2_file}\"\n",
    "        os.system(command)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function that will create an overlay video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_video(combo_images, action_dict, filename, fps):\n",
    "    print(f\"Generating a video of the overlay for {filename}\")\n",
    "    images = []\n",
    "    for _, action_index in action_dict.items():\n",
    "        image_path = combo_images[action_index]\n",
    "        with Image.open(image_path) as image:\n",
    "            images.append(np.array(image))\n",
    "    clip = ImageSequenceClip(images, fps=fps)\n",
    "    clip = clip.set_duration(len(action_dict) / fps)\n",
    "    clip = clip.set_position(('center', 'center'))\n",
    "    clip.write_videofile(filename, fps=fps, codec='mpeg4')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our main function that will call our helper functions in turn, automating the process of generating files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automate():\n",
    "    for steps in range(check_freq, total_timesteps+1, check_freq):\n",
    "        marlio = f\"best_model_{steps}.zip\"\n",
    "        model = PPO.load(f\"./train/{marlio}\")\n",
    "        print(f\"using {marlio}\")\n",
    "        print()\n",
    "\n",
    "        action_dict = pred(model)\n",
    "        filename = \"./overlays/\" + marlio.split(\".\")[0] + \".mp4\"\n",
    "        generate_video(combo_images, action_dict, filename, 60)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's automate the boring stuff!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bk2_to_mp4()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b3ded1ccb95c1d9bd405e7b823d9e85424cde40fbb5985eb47e999ef50e15b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
