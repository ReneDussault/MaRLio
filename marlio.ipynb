{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Reinforcement Learning for Super Mario World and the potential of RL models in Nuclear Fusion\n",
    "\n",
    "#### Project Overview\n",
    "The main goal of this project is to use deep reinforcement learning (DRL) to train a model to play Super Mario World on the SNES.  \n",
    "The project will demonstrate the ability of RL models to use data as inputs and learn to prevent bad outcomes (from the reward function).  \n",
    "Additionally, the proposal will explore the potential of DRL models in controlling devices in IoT, specifically in the context of nuclear fusion (to be seen).  \n",
    "(The project will be divided into two parts: (1) training a DRL model to play Super Mario World, and (2) exploring the potential of DRL in controlling devices in IoT.)  \n",
    "\n",
    "#### Data Description\n",
    "The data for this project will come from the OpenAI Gym Retro environment, which provides an emulator for Super Mario World.  \n",
    "The dataset consists of frames from the game, along with actions taken by the model and the corresponding rewards.  \n",
    "The dataset will need to be preprocessed to extract features that are relevant to the DRL model.  \n",
    "\n",
    "#### Methodology\n",
    "The DRL model will be trained using the Proximal Policy Optimization (PPO) algorithm.  \n",
    "The DRL model will be evaluated using a set of metrics, including the average score achieved and the number of deaths.  \n",
    "For the exploration of the potential of DRL in controlling devices in IoT, the project will use a simulated environment to test the model's ability to control the parameters of the system.  \n",
    "\n",
    "#### The timeline for completing the project is as follows:\n",
    "Week 1: Collect and preprocess data for Super Mario World  \n",
    "Week 2: Train and evaluate the DRL model on Super Mario World  \n",
    "Week 3: Visualize the model's progress  \n",
    "Week 4: Explore the potential of DRL in controlling IoT devices  \n",
    "\n",
    "#### Results Interpretation\n",
    "The results of the Super Mario World model will be interpreted in terms of the average score achieved and the number of deaths.  \n",
    "The results of the exploration of DRL in IoT will be interpreted in terms of the model's ability to control the parameters of the system and prevent failures.  \n",
    "The interpretation of the results will be presented in a dashboard, which will include graphs and visualizations to make the results more interpretable.  \n",
    "\n",
    "#### Deployment and Delivery\n",
    "The DRL model will be deployed as a standalone application that can be run on a desktop computer.  \n",
    "The application will be delivered with a user manual that provides instructions on how to use the application.  \n",
    "Additionally, a dashboard will be provided to showcase the results of the project.  \n",
    "\n",
    "#### Conclusion\n",
    "The project will demonstrate the ability of DRL models to learn from data and prevent bad outcomes.  \n",
    "The exploration of the potential of DRL in controlling devices in IoT will provide insights into how this technology can be applied in real-world scenarios.  \n",
    "Overall, this project will showcase the potential of DRL in solving complex problems and preventing failures of complex systems.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import retro\n",
    "\n",
    "import gym.envs.classic_control as control\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = retro.make(game=\"SuperMarioWorld-Snes\", state=\"YoshiIsland1\", use_restricted_actions=retro.Actions.FILTERED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = env.action_space.sample()\n",
    "print(action)\n",
    "env.get_action_meaning(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, reward, terminated, info = env.step(action)\n",
    "print(f\"score: {reward}\\nterminated: {terminated}\\ninfo: {info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = True\n",
    "for step in range(100000): \n",
    "    if done: \n",
    "        env.reset()\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    # time.sleep(1/100)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess: grayscale, vectorize and framestacking\n",
    "env = GrayScaleObservation(env, keep_dim=True)\n",
    "env = DummyVecEnv([lambda:env])\n",
    "env = VecFrameStack(env, 4, channels_order=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(state[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, done, info = env.step([env.action_space.sample()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,16))\n",
    "for idx in range(state.shape[3]):\n",
    "    plt.subplot(1,4,idx+1)\n",
    "    plt.imshow(state[0][:,:,idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback helper function\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, \"best_model_{}\".format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = \"./train/\"\n",
    "LOG_DIR = \"./logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback\n",
    "callback = TrainAndLoggingCallback(check_freq=1000000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\"CnnPolicy\", env, verbose=1, tensorboard_log=LOG_DIR, learning_rate=0.0003, \n",
    "            n_steps=128, n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=10000000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"./train/best_model_1000000.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# game loop for predict\n",
    "state = env.reset()\n",
    "\n",
    "while True:\n",
    "    action, _ = model.predict(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
