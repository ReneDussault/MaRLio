{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic imports for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports that will emulate the game and an action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.envs.classic_control.rendering import SimpleImageViewer\n",
    "import retro\n",
    "import pygame\n",
    "\n",
    "from gym import Wrapper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports for our PPO algorithm (Proximal Policy Optimization) and for pre-processing (grayscale and frame stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GPU for training (with AMD's ROCm), checking if we have access to Cuda (at least 4 times faster training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hijacking the default action space with a wrapper that will allow us to\n",
    "### create our own combos of buttons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discretizer(gym.ActionWrapper):\n",
    "\n",
    "    def __init__(self, env, combos):\n",
    "        super().__init__(env)\n",
    "        assert isinstance(env.action_space, gym.spaces.MultiBinary)\n",
    "        buttons = env.unwrapped.buttons\n",
    "        self._decode_discrete_action = []\n",
    "        for combo in combos:\n",
    "            arr = np.array([False]*env.action_space.n)\n",
    "            for button in combo:\n",
    "                arr[buttons.index(button)] = True\n",
    "            self._decode_discrete_action.append(arr)\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(len(self._decode_discrete_action))\n",
    "\n",
    "    def action(self, act):\n",
    "        return self._decode_discrete_action[act].copy()\n",
    "\n",
    "\n",
    "class MaRLioDiscretizer(Discretizer):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env=env, \n",
    "                         combos=[\n",
    "            [],\n",
    "            [\"RIGHT\"],\n",
    "            [\"RIGHT\",\"Y\"],\n",
    "            [\"RIGHT\",\"A\"],\n",
    "            [\"RIGHT\",\"Y\",\"A\"],\n",
    "            [\"RIGHT\",\"Y\",\"B\"],\n",
    "            [\"LEFT\",\"Y\"],\n",
    "            [\"LEFT\",\"Y\",\"A\"],\n",
    "            [\"LEFT\",\"Y\",\"B\"],\n",
    "            [\"A\"],\n",
    "            [\"B\"]\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping our combos in a list for an easy visualization using indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combos = [\n",
    "    [],\n",
    "    [\"RIGHT\"],\n",
    "    [\"RIGHT\",\"Y\"],\n",
    "    [\"RIGHT\",\"A\"],\n",
    "    [\"RIGHT\",\"Y\",\"A\"],\n",
    "    [\"RIGHT\",\"Y\",\"B\"],\n",
    "    [\"LEFT\",\"Y\"],\n",
    "    [\"LEFT\",\"Y\",\"A\"],\n",
    "    [\"LEFT\",\"Y\",\"B\"],\n",
    "    [\"A\"],\n",
    "    [\"B\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's add a controller dictionary that maps our buttons to our overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_images = {\n",
    "    0: \"./controller/blank.png\",\n",
    "    1: \"./controller/right.png\",\n",
    "    2: \"./controller/righty.png\",\n",
    "    3: \"./controller/righta.png\",\n",
    "    4: \"./controller/rightya.png\",\n",
    "    5: \"./controller/rightyb.png\",\n",
    "    6: \"./controller/lefty.png\",\n",
    "    7: \"./controller/leftya.png\",\n",
    "    8: \"./controller/leftyb.png\",\n",
    "    9: \"./controller/a.png\",\n",
    "    10: \"./controller/b.png\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the path of our custom data.json and scenario.json files into variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data.json\"\n",
    "scenario_path = \"./scenario.json\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data.json will have memory addresses for things that we want to keep track of\n",
    "### and will be accessed from the info variable once we run the environment\n",
    "### scenario.json will be our way of calculating MaRLio's reward\n",
    "### Let's see how they look like. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data.json\n",
    "\n",
    "````\n",
    "{\n",
    "  \"info\": {\n",
    "    \"checkpoint\": {\n",
    "      \"address\": 5070,\n",
    "      \"type\": \"|i1\"\n",
    "    },\n",
    "    \"coins\": {\n",
    "      \"address\": 8261055,\n",
    "      \"type\": \"|u1\"\n",
    "    },\n",
    "    \"end_of_level\": {\n",
    "      \"address\": 8259846,\n",
    "      \"type\": \"|i1\"\n",
    "    },\n",
    "    \"lives\": {\n",
    "      \"address\": 8261054,\n",
    "      \"type\": \"|i1\"\n",
    "    },\n",
    "    \"powerups\": {\n",
    "      \"address\": 25,\n",
    "      \"type\": \"|i1\"\n",
    "    },\n",
    "    \"score\": {\n",
    "      \"address\": 8261428,\n",
    "      \"type\": \"<u4\"\n",
    "    },\n",
    "    \"xpos\": {\n",
    "      \"address\": 148,\n",
    "      \"type\": \"<u2\"\n",
    "    },\n",
    "    \"ypos\": {\n",
    "      \"address\": 114,\n",
    "      \"type\": \"<u4\"\n",
    "    },\n",
    "    \"yoshi_coins\": {\n",
    "      \"address\": 8262690,\n",
    "      \"type\": \"<u4\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "````"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scenario.json\n",
    "\n",
    "````\n",
    "{\n",
    "  \"crop\": [\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0\n",
    "  ],\n",
    "  \"done\": {\n",
    "    \"variables\": {\n",
    "      \"lives\": {\n",
    "        \"op\": \"equal\",\n",
    "        \"reference\": -1\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"reward\": {\n",
    "    \"variables\": {\n",
    "      \"score\": {\n",
    "        \"reward\": 1.0\n",
    "      },\n",
    "      \"coins\": {\n",
    "        \"reward\": 1.0\n",
    "      },\n",
    "      \"yoshi_coins\": {\n",
    "        \"reward\": 1.0\n",
    "      },\n",
    "      \"xpos\": {\n",
    "        \"op\": \"greater\",\n",
    "        \"reference\": \"last\",\n",
    "        \"reward\": 10\n",
    "      },\n",
    "      \"ypos\": {\n",
    "        \"op\": \"less\",\n",
    "        \"reference\": \"last\",\n",
    "        \"reward\": 10\n",
    "      },\n",
    "      \"xpos\": {\n",
    "        \"op\": \"less\",\n",
    "        \"reference\": \"last\",\n",
    "        \"penalty\": 0.1\n",
    "      },\n",
    "      \"lives\": {\n",
    "        \"op\": \"less\",\n",
    "        \"reference\": \"last\",\n",
    "        \"penalty\": 1000.0\n",
    "      },\n",
    "      \"end_of_level\": {\n",
    "        \"op\": \"equal\",\n",
    "        \"reference\": 1,\n",
    "        \"reward\": 1000000.0\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "````"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's load our game, Super Mario World (SMW) from the Super Nintendo Entertainment System (SNES)\n",
    "### using scenario and info kwargs, we can load our custom data.json and scenario.json\n",
    "### using record kwarg, we can save the training sessions for future viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train\n",
    "\n",
    "# env = retro.make(game=\"SuperMarioWorld-Snes\", state=\"YoshiIsland2\", record=\"./recordings/train/\", scenario=scenario_path, info=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pred\n",
    "\n",
    "# env = retro.make(game=\"SuperMarioWorld-Snes\", state=\"YoshiIsland2\", record=\"./recordings/pred/\", scenario=scenario_path, info=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for video only, no recording\n",
    "\n",
    "env = retro.make(game=\"SuperMarioWorld-Snes\", state=\"YoshiIsland2\", scenario=scenario_path, info=data_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is where we wrap the environment in our own action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_env = MaRLioDiscretizer(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = disc_env.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the index it returns and what combo we get from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = disc_env.action_space.sample()\n",
    "print(action)\n",
    "combos[action]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can use that action inside our environment, one frame at a time\n",
    "### and we can use tuple unpacking, to get access to some important information about our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, reward, terminated, info = disc_env.step(action)\n",
    "print(f\"score: {reward}\\nterminated: {terminated}\\ninfo: {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The image we get is a 3-dimensional tensor with a height of 224 pixels, a width of 256 pixels, and 3 color channels (RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize that frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# game loop with random actions\n",
    "# state = disc_env.reset()\n",
    "\n",
    "# done = False\n",
    "# while not done:\n",
    "#     action = disc_env.action_space.sample()\n",
    "#     state, reward, done, info = disc_env.step(action)\n",
    "#     disc_env.render()\n",
    "#     time.sleep(0.005)\n",
    "# disc_env.render(close=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's pre-process our images\n",
    "### First, grayscale. that'll reduce the size of our environment by 66%\n",
    "### Second, let's vectorize everything\n",
    "### Third, let's stack 4 frames, our model will get a sense of momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_env = GrayScaleObservation(disc_env, keep_dim=True)\n",
    "disc_env = DummyVecEnv([lambda:disc_env])\n",
    "disc_env = VecFrameStack(disc_env, 4, channels_order=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = disc_env.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at a frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(state[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This time, the (1, 224, 256, 4) shape indicates that the observation is a stack of 4 frames, \n",
    "### where each frame is a \"color\" (grayscale, 1 RGB channel) image with a resolution of 224x256 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking 4 steps, so we don't have blank frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(3):\n",
    "    state, reward, done, info = disc_env.step([disc_env.action_space.sample()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize our stack of frames, with random actions from our list of combos\n",
    "### We should see some movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,16))\n",
    "for idx in range(state.shape[3]):\n",
    "    plt.subplot(1,4,idx+1)\n",
    "    plt.imshow(state[0][:,:,idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = disc_env.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a class that will create our logs and save our model at every check_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback helper function\n",
    "\n",
    "# logs and model saving\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, \"best_model_{}\".format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating variables that points to our paths for saving the model and the logs (tensorboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = \"./train/\"\n",
    "LOG_DIR = \"./logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=100_000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping track of some hyperparameters for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda params\n",
    "\n",
    "learning_rate = lambda f:f*0.000001\n",
    "# learning_rate = 0.000001\n",
    "n_steps = 128\n",
    "clip_range = 0.2\n",
    "ent_coef = 0.01"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance of our PPO algorithm, with the rest of the hyperparameters set to default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    \"CnnPolicy\", \n",
    "    disc_env, \n",
    "    learning_rate=learning_rate, \n",
    "    n_steps=n_steps, \n",
    "    batch_size=64, \n",
    "    n_epochs=10, \n",
    "    gamma=0.99, \n",
    "    gae_lambda=0.95, \n",
    "    clip_range=clip_range, \n",
    "    clip_range_vf=None, \n",
    "    normalize_advantage=True, \n",
    "    ent_coef=ent_coef, \n",
    "    vf_coef=0.5, \n",
    "    max_grad_norm=0.5, \n",
    "    use_sde=False, \n",
    "    sde_sample_freq=-1, \n",
    "    target_kl=None,  \n",
    "    tensorboard_log=LOG_DIR, \n",
    "    policy_kwargs=None, \n",
    "    verbose=0, \n",
    "    seed=42, \n",
    "    device='auto', \n",
    "    _init_setup_model=True\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is where MaRLio starts his adventure (training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(\n",
    "    total_timesteps=1_000_000, \n",
    "    progress_bar=True, \n",
    "    tb_log_name=\"ppo\",\n",
    "    callback=[\n",
    "    callback\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's add a powerup dictionnary, we will see what powerups MaRLio has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "powerup_dict = {\n",
    "    0: \"small\",\n",
    "    1: \"big\",\n",
    "    2: \"cape\",\n",
    "    3: \"fire\"\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's load the models and predict the environment with the models' knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marlio = \"best_model_400000.zip\"\n",
    "model = PPO.load(f\"./train/{marlio}\")\n",
    "print(f\"using {marlio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = disc_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# game loop for predict\n",
    "\n",
    "total_reward = 0\n",
    "action_list = [0]\n",
    "powerup = \"\"\n",
    "check = False\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "\n",
    "    action, _ = model.predict(state)\n",
    "    state, reward, done, info = disc_env.step(action)\n",
    "\n",
    "    if action[0] not in action_list:\n",
    "        action_list.pop()\n",
    "        action_list.append(action)\n",
    "        # print(combos[action_list[0]])\n",
    "\n",
    "    if info[0][\"powerups\"] != powerup:\n",
    "        powerup = info[0][\"powerups\"]\n",
    "        print(powerup_dict.get(powerup, \"Invalid integer value\"))\n",
    "\n",
    "    if info[0][\"checkpoint\"] == 1 and not check:\n",
    "        check = True\n",
    "        print(\"Midlevel Checkpoint Reached: +500k reward\")\n",
    "\n",
    "    if info[0][\"end_of_level\"] == 1:\n",
    "        print(\"End of level Reached: +1m reward\")\n",
    "\n",
    "    total_reward += reward\n",
    "    disc_env.render()\n",
    "    time.sleep(0.004)\n",
    "\n",
    "print(\"Total reward:\", total_reward)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the ressources used for the project\n",
    "\n",
    "# videos\n",
    "\n",
    "# https://www.youtube.com/watch?v=2eeYqJ0uBKE\n",
    "# https://www.youtube.com/watch?v=qv6UVOQ0F44\n",
    "# https://www.youtube.com/watch?v=fkvc9Qc5ycE\n",
    "# https://www.youtube.com/@LucasThompson/videos\n",
    "# https://www.youtube.com/watch?v=PxoG0A2QoFs\n",
    "# https://www.youtube.com/playlist?list=PLTWFMbPFsvz3CeozHfeuJIXWAJMkPtAdS\n",
    "# https://www.youtube.com/watch?v=yO4CiiigHjA\n",
    "# https://www.youtube.com/playlist?list=PLTWFMbPFsvz2LIR7thpuU738FcRQbR_8I\n",
    "# https://www.youtube.com/watch?v=MEt6rrxH8W4&t=7s\n",
    "\n",
    "\n",
    "# github\n",
    "\n",
    "# https://github.com/Farama-Foundation/Gymnasium\n",
    "# https://github.com/MatPoliquin/stable-retro\n",
    "# https://github.com/openai/gym\n",
    "# https://github.com/openai/retro#readme\n",
    "# https://github.com/openai/retro/tree/master/retro/data/stable\n",
    "# https://github.com/CodeReclaimers/neat-python\n",
    "# https://github.com/healthpotionstudios/MarioWorldAI-NEAT\n",
    "# https://github.com/szmyty/OpenAI-Retro-SuperMarioWorld-SNES\n",
    "# https://github.com/nicknochnack/MarioRL/blob/main/Mario%20Tutorial.ipynb\n",
    "\n",
    "\n",
    "# docs\n",
    "\n",
    "# https://retro.readthedocs.io/en/latest/getting_started.html#create-a-gym-environment\n",
    "# https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html\n",
    "# https://stable-baselines3.readthedocs.io/en/master/\n",
    "# https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/\n",
    "# https://openai.com/research/gym-retro\n",
    "\n",
    "\n",
    "# misc\n",
    "\n",
    "# https://www.snesmaps.com/maps/SuperMarioWorld/SuperMarioWorld.html\n",
    "# https://www.smwcentral.net/?p=memorymap&game=smw&region=ram\n",
    "# https://pastebin.com/ZZmSNaHX\n",
    "# https://www.videogames.ai/2022/09/01/RX-6700s-Machine-Learning-ROCm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario.json -> https://retro.readthedocs.io/en/latest/integration.html?highlight=scenario#scenario-scenario-json\n",
    "# metadata.json -> https://retro.readthedocs.io/en/latest/integration.html?highlight=scenario#metadata-json\n",
    "# data.json -> https://retro.readthedocs.io/en/latest/integration.html?highlight=scenario#data-json\n",
    "\n",
    "# appendix for operations -> https://retro.readthedocs.io/en/latest/integration.html?highlight=scenario#appendix-operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert .bk2 to .mp4\n",
    "\n",
    "# https://retro.readthedocs.io/en/latest/python.html#render-to-video\n",
    "# python3.8 -m retro.scripts.playback_movie SuperMarioWorld-Snes-YoshiIsland3-001490.bk2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep an eye on GPU while training (AMD using ROCm)\n",
    "\n",
    "# watch -n 1 rocm-smi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below code not used for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay_img = cv2.imread(combo_images[5], cv2.IMREAD_UNCHANGED)\n",
    "# overlay_imga = cv2.cvtColor(overlay_img, cv2.COLOR_BGR2RGBA)\n",
    "# merged = cv2.addWeighted(overlay_imga, 1, state[0], 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.matshow(overlay_imga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.matshow(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComboImageWrapper(Wrapper):\n",
    "    def __init__(self, env, images_path):\n",
    "        super().__init__(env)\n",
    "        self.images_path = images_path\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        observation = self.env.reset(**kwargs)\n",
    "        return observation\n",
    "\n",
    "    def step(self, action):\n",
    "        overlay_img = cv2.imread(self.images_path[int(action)], cv2.IMREAD_UNCHANGED)\n",
    "        overlay_imga = cv2.cvtColor(overlay_img, cv2.COLOR_BGR2RGBA)\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        merged = cv2.addWeighted(overlay_imga, 1, observation[0], 1, 0)\n",
    "        return merged.reshape((1, merged.shape[0], merged.shape[1], merged.shape[2])), reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.init()\n",
    "win = pygame.display.set_mode((800,600))\n",
    "clock = pygame.time.Clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = disc_env.reset()\n",
    "\n",
    "total_reward = 0\n",
    "action_list = [0]\n",
    "powerup = \"\"\n",
    "check = False\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    action, _ = model.predict(state)\n",
    "    overlay_img = cv2.imread(combo_images[int(action)], cv2.IMREAD_UNCHANGED)\n",
    "    overlay_imga = cv2.cvtColor(overlay_img, cv2.COLOR_BGR2RGBA)\n",
    "    merged = cv2.addWeighted(overlay_imga, 1, state[0], 1, 0)\n",
    "\n",
    "    img = pygame.image.frombuffer(merged.tobytes(), merged.shape[1::-1], \"RGBA\")\n",
    "    img = pygame.transform.scale(img, (800,600))\n",
    "    win.blit(img,(0,0))\n",
    "    pygame.display.flip()\n",
    "\n",
    "\n",
    "    if info[0][\"powerups\"] != powerup:\n",
    "        powerup = info[0][\"powerups\"]\n",
    "        print(powerup_dict.get(powerup, \"Invalid integer value\"))\n",
    "\n",
    "    if info[0][\"checkpoint\"] == 1 and not check:\n",
    "        check = True\n",
    "        print(\"Midlevel Checkpoint Reached: +500k reward\")\n",
    "\n",
    "    if info[0][\"end_of_level\"] == 1:\n",
    "        print(\"End of level Goal Reached: +1m reward\")\n",
    "\n",
    "    total_reward += reward\n",
    "    # env.render()\n",
    "    state, reward, done, info = disc_env.step(action)\n",
    "    # clock.tick(40)\n",
    "    time.sleep(0.004)\n",
    "\n",
    "print(\"Total reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible playback code below\n",
    "\n",
    "movie = retro.Movie(\"./recordings/train/SuperMarioWorld-Snes-YoshiIsland3-000008.bk2\")\n",
    "movie.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = retro.make(\n",
    "    game=movie.get_game(),\n",
    "    state=None,\n",
    "    # bk2s can contain any button presses, so allow everything\n",
    "    use_restricted_actions=retro.Actions.ALL,\n",
    "    players=movie.players,\n",
    ")\n",
    "env.initial_state = movie.get_state()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while movie.step():\n",
    "    keys = []\n",
    "    for p in range(movie.players):\n",
    "        for i in range(env.num_buttons):\n",
    "            keys.append(movie.get_key(i, p))\n",
    "    env.step(keys)\n",
    "    env.render()\n",
    "    # time.sleep(0.004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_video(env, policy, out_directory, fps=30):\n",
    "    images = []\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "    images.append(img)\n",
    "    while not done:\n",
    "        state = torch.Tensor(state).to(device)\n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action, _, _, _ = policy.get_action_and_value(state)\n",
    "        state, reward, done, info = env.step(\n",
    "            action.cpu().numpy()\n",
    "        )  # We directly put next_state = state for recording logic\n",
    "        img = env.render(mode=\"rgb_array\")\n",
    "        images.append(img)\n",
    "    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b3ded1ccb95c1d9bd405e7b823d9e85424cde40fbb5985eb47e999ef50e15b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
